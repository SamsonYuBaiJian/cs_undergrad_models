{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI homework 2 - neural networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUbEmuvZJxlI",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch - homework 2: neural networks\n",
        "\n",
        "-- Prof. Dorien Herremans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efS07mO7J6AR",
        "colab_type": "text"
      },
      "source": [
        "Please run the whole notebook with your code and submit the `.ipynb` file on eDimension that includes your answers [so after you run it]. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJpzFaX0J6Zz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "601a8fde-225d-489b-be0b-24febe59df16"
      },
      "source": [
        "from termcolor import colored\n",
        "\n",
        "student_number=\"1002819\"\n",
        "student_name=\"Samson Yu Bai Jian\"\n",
        "\n",
        "print(colored(\"Homework by \"  + student_name + ', number: ' + student_number,'red'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mHomework by Samson Yu Bai Jian, number: 1002819\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xDkwBg8LKQ_",
        "colab_type": "text"
      },
      "source": [
        " ## Question 1 -- XOR neural network [3pts]\n",
        "\n",
        "a) Train an (at least) 2-layer neural network that can solve the XOR problem. \n",
        "\n",
        "b) Check the predictions resulting from your model in the second code box below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BINvhm-PLKak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "895dbeb5-96d5-4b88-8e7e-c1b4d89abb1c"
      },
      "source": [
        "# load your data\n",
        "import torch\n",
        "\n",
        "X = torch.Tensor([[0,0],[0,1], [1,0], [1,1]])\n",
        "Y = torch.LongTensor([0,1,1,0]).view(-1,1)\n",
        "\n",
        "# name your model xor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden, hidden_dim, dropout):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        \n",
        "        assert num_hidden > 0\n",
        "        self.hidden_layers = nn.ModuleList([])\n",
        "        self.hidden_layers.append(nn.Linear(input_size, hidden_dim))\n",
        "        for i in range(num_hidden - 1):\n",
        "            self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_projection = nn.Linear(hidden_dim, num_classes)\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.nonlinearity(x)\n",
        "      \n",
        "        out = self.output_projection(x)\n",
        "        return out\n",
        "\n",
        "def xor():\n",
        "    num_outputs = 2\n",
        "    num_input_features = 2\n",
        "    num_hidden = 2\n",
        "    hidden_dim = 5\n",
        "    dropout = 0\n",
        "\n",
        "    model = FeedForwardNN(num_input_features, num_outputs, num_hidden, hidden_dim, dropout)\n",
        "    return model\n",
        "\n",
        "xor = xor()\n",
        "\n",
        "# define your model loss function, optimizer, etc. \n",
        "lr_rate = 0.02\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(xor.parameters(), lr=lr_rate)\n",
        "\n",
        "# train the model\n",
        "import numpy as np\n",
        "\n",
        "epochs = 2000\n",
        "steps = X.size(0)\n",
        "\n",
        "for i in range(epochs):\n",
        "    for j in range(steps):\n",
        "        data_point = np.random.randint(X.size(0))\n",
        "\n",
        "        x_var = torch.Tensor(X[data_point]).unsqueeze(0)\n",
        "        y_var = torch.LongTensor(Y[data_point])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = xor(x_var)\n",
        "\n",
        "        loss = loss_function(y_hat, y_var)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print (\"Epoch: {0}, Loss: {1}, \".format(i, loss.data.numpy()))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.5926584005355835, \n",
            "Epoch: 100, Loss: 0.6802399754524231, \n",
            "Epoch: 200, Loss: 0.5889790058135986, \n",
            "Epoch: 300, Loss: 0.5438827872276306, \n",
            "Epoch: 400, Loss: 0.042589422315359116, \n",
            "Epoch: 500, Loss: 0.022974850609898567, \n",
            "Epoch: 600, Loss: 0.1682976931333542, \n",
            "Epoch: 700, Loss: 0.007836077362298965, \n",
            "Epoch: 800, Loss: 0.08127047121524811, \n",
            "Epoch: 900, Loss: 0.06406918913125992, \n",
            "Epoch: 1000, Loss: 0.0047372253611683846, \n",
            "Epoch: 1100, Loss: 0.0037967516109347343, \n",
            "Epoch: 1200, Loss: 0.002916489727795124, \n",
            "Epoch: 1300, Loss: 0.0014448452275246382, \n",
            "Epoch: 1400, Loss: 0.030779751017689705, \n",
            "Epoch: 1500, Loss: 0.0012076949933543801, \n",
            "Epoch: 1600, Loss: 0.0017921352991834283, \n",
            "Epoch: 1700, Loss: 0.0008364992681890726, \n",
            "Epoch: 1800, Loss: 0.0014757943572476506, \n",
            "Epoch: 1900, Loss: 0.019848771393299103, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51Ra1T6n2r_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "9a35a14c-7e06-446e-d7e6-b67e4e3776ff"
      },
      "source": [
        "# test your model using the following functions (make sure the output is printed and saved when you submit this notebook):\n",
        "# depending on how you defined your network you may need to slightly tweek the below prediction function\n",
        "\n",
        "test = [[0,0],[0,1],[1,1],[1,0]]\n",
        "\n",
        "for trial in test: \n",
        "  Xtest = torch.Tensor(trial)\n",
        "  y_hat = xor(Xtest)\n",
        "  prediction = np.argmax(y_hat.detach().numpy(), axis=0)\n",
        "  print(\"{0} xor {1} = {2}\".format(int(Xtest[0]), int(Xtest[1]), prediction))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 xor 0 = 0\n",
            "0 xor 1 = 1\n",
            "1 xor 1 = 0\n",
            "1 xor 0 = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqIqD5ZzyUOW",
        "colab_type": "text"
      },
      "source": [
        "## Question 2  [2pts]\n",
        "\n",
        "Imagine a neural network model for a multilabel classification task. \n",
        "\n",
        "a) Which loss function should you use?\n",
        "\n",
        "b) The resulting trained modal has a high variance error. Give 4 possible solutions to improve the model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzye9G18PQ0c",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "[your answer here, no coding required]\n",
        "\n",
        "* answer A\n",
        "nn.BCELoss, since we want to do binary classification on each output node. For example, if there are 10 classes, there will be 10 output nodes, and we want to check for each class if it is part of the input. Hence, we will do binary classification on each output node.\n",
        "\n",
        "* answer B\n",
        "  - Increase training dataset size.\n",
        "  - Decrease model size (e.g. number of layers or number of parameters).\n",
        "  - Add early stopping.\n",
        "  - Add regularisation.\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcceOSnjjSHf",
        "colab_type": "text"
      },
      "source": [
        "## Question 3 - Improve hit classification [5pts]\n",
        "\n",
        "Remember the hit predicton dataset from last week? \n",
        "\n",
        "a) Improve the model using a multiplayer perceptron. \n",
        "\n",
        "b) Make sure to run your models on the GPU. \n",
        "\n",
        "c) Tweek the hyperparameters such as number of nodes or layers, or other. Show two possible configurations and explain which works better and very briefly explain why this may be the case. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-jkJDTdjSRX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "405f09dd-7403-46dd-e16b-1228810b8d8a"
      },
      "source": [
        "# code your model 1\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_hidden, hidden_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        assert num_hidden > 0\n",
        "        self.hidden_layers = nn.ModuleList([])\n",
        "        self.hidden_layers.append(nn.Linear(input_size, hidden_dim))\n",
        "        for i in range(num_hidden - 1):\n",
        "            self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.output_projection = nn.Linear(hidden_dim, num_classes)\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.nonlinearity(x)\n",
        "      \n",
        "        out = self.output_projection(x)\n",
        "        out_distribution = torch.sigmoid(out)\n",
        "\n",
        "        return out_distribution\n",
        "\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.data[index]\n",
        "        y = self.labels[index]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "train = pd.read_csv('https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030training.csv')\n",
        "labels = train.iloc[:,-1]\n",
        "train = train.drop('Topclass1030', axis=1)\n",
        "traindata = torch.Tensor(train.values)\n",
        "trainlabels = torch.Tensor(labels.values).view(-1,1)\n",
        "dataset = Dataset(traindata, trainlabels)\n",
        "\n",
        "# model1 parameters\n",
        "num_outputs = 1\n",
        "num_input_features = train.shape[-1]\n",
        "num_hidden = 2\n",
        "hidden_dim = 50\n",
        "\n",
        "model1 = MLP(num_input_features, num_outputs, num_hidden, hidden_dim).to(device)\n",
        "\n",
        "# training parameters\n",
        "epochs = 1000\n",
        "lr_rate = 0.02\n",
        "criterion = nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model1.parameters(), lr=lr_rate)\n",
        "\n",
        "# dataloader parameters\n",
        "params = {'batch_size': 32,\n",
        "          'shuffle': True}\n",
        "dataloader = torch.utils.data.DataLoader(dataset, **params)\n",
        "\n",
        "for i in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch_data, batch_labels in dataloader:\n",
        "        X = batch_data.to(device)\n",
        "        y = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model1(X)\n",
        "\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.cpu().data.numpy()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print (\"Epoch: {0}, Loss: {1}, \".format(i, epoch_loss / len(dataloader)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.6821933388710022, \n",
            "Epoch: 100, Loss: 0.45815915682099084, \n",
            "Epoch: 200, Loss: 0.19358438795263117, \n",
            "Epoch: 300, Loss: 0.025462186979976566, \n",
            "Epoch: 400, Loss: 0.009691712742840702, \n",
            "Epoch: 500, Loss: 0.0045931234066797924, \n",
            "Epoch: 600, Loss: 0.003005152515305037, \n",
            "Epoch: 700, Loss: 0.0022362439723854714, \n",
            "Epoch: 800, Loss: 0.0017538869922811334, \n",
            "Epoch: 900, Loss: 0.0014217824103649366, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIDPTKcFkETc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c3f788e1-cbcc-448b-a716-048f76d7b0f4"
      },
      "source": [
        "# evaluate model 1 (called model1 here)\n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "def run_evaluation(my_model):\n",
        "\n",
        "#   test = pd.read_csv('/content/herremans_hit_1030test.csv')\n",
        "  test = pd.read_csv('https://dorax.s3.ap-south-1.amazonaws.com/herremans_hit_1030test.csv')\n",
        "  labels = test.iloc[:,-1]\n",
        "  test = test.drop('Topclass1030', axis=1)\n",
        "  testdata = torch.Tensor(test.values)\n",
        "  testlabels = torch.Tensor(labels.values).view(-1,1)\n",
        "\n",
        "  TP = 0\n",
        "  TN = 0\n",
        "  FN = 0\n",
        "  FP = 0\n",
        "\n",
        "  for i in range(0, testdata.size()[0]): \n",
        "    # print(testdata[i].size())\n",
        "    Xtest = torch.Tensor(testdata[i]).to(device)\n",
        "    y_hat = my_model(Xtest)\n",
        "    \n",
        "    if y_hat > 0.5:\n",
        "      prediction = 1\n",
        "    else: \n",
        "      prediction = 0\n",
        "\n",
        "    if (prediction == testlabels[i]):\n",
        "      if (prediction == 1):\n",
        "        TP += 1\n",
        "      else: \n",
        "        TN += 1\n",
        "\n",
        "    else:\n",
        "      if (prediction == 1):\n",
        "        FP += 1\n",
        "      else: \n",
        "        FN += 1\n",
        "\n",
        "  print(\"True Positives: {0}, True Negatives: {1}\".format(TP, TN))\n",
        "  print(\"False Positives: {0}, False Negatives: {1}\".format(FP, FN))\n",
        "  rate = TP/(FN+TP)\n",
        "  print(\"Class specific accuracy of correctly predicting a hit song is {0}\".format(rate))\n",
        "\n",
        "run_evaluation(model1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positives: 38, True Negatives: 16\n",
            "False Positives: 13, False Negatives: 12\n",
            "Class specific accuracy of correctly predicting a hit song is 0.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xghPDDNmkHn2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "f4985148-1446-4eac-b2eb-cc92fb009100"
      },
      "source": [
        "# code your model 2\n",
        "# model2 parameters\n",
        "num_outputs = 1\n",
        "num_input_features = train.shape[-1]\n",
        "num_hidden = 5\n",
        "hidden_dim = 50\n",
        "\n",
        "model2 = MLP(num_input_features, num_outputs, num_hidden, hidden_dim).to(device)\n",
        "\n",
        "# training parameters\n",
        "epochs = 1000\n",
        "lr_rate = 0.02\n",
        "criterion = nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model2.parameters(), lr=lr_rate)\n",
        "\n",
        "for i in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch_data, batch_labels in dataloader:\n",
        "        X = batch_data.to(device)\n",
        "        y = batch_labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model2(X)\n",
        "\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.cpu().data.numpy()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print (\"Epoch: {0}, Loss: {1}, \".format(i, epoch_loss / len(dataloader)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.7034374908967451, \n",
            "Epoch: 100, Loss: 0.6353667432611639, \n",
            "Epoch: 200, Loss: 0.5634772940115496, \n",
            "Epoch: 300, Loss: 0.15108455514365976, \n",
            "Epoch: 400, Loss: 0.08429935269735077, \n",
            "Epoch: 500, Loss: 0.016981393453368746, \n",
            "Epoch: 600, Loss: 0.0013241947480392728, \n",
            "Epoch: 700, Loss: 0.0005806320272809403, \n",
            "Epoch: 800, Loss: 0.00034530234353786165, \n",
            "Epoch: 900, Loss: 0.00024352074301119004, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAIifiHJkHyW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "38ff3025-de75-498c-9b3a-240543bdf1d4"
      },
      "source": [
        "# evaluate model 2 (called model2 here)\n",
        "\n",
        "run_evaluation(model2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Positives: 42, True Negatives: 16\n",
            "False Positives: 13, False Negatives: 8\n",
            "Class specific accuracy of correctly predicting a hit song is 0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPsxbT0KkGs1",
        "colab_type": "text"
      },
      "source": [
        "Which works better and why do you think this may be (very briefly)? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GzjI77HkSwH",
        "colab_type": "text"
      },
      "source": [
        "**[your answer here, also please summarise the differences between your two models]**\n",
        "model2 works better, with a class specific accuracy of 0.84 as compared to model1's 0.76.\n",
        "The main difference is that model2 has more hidden layers (specifically 5 layers as compared to model1's 2). This potentially allows model2 to reduce its bias error (i.e. underfitting) by learning more complex representations of the data. This can also be seen in how training loss is lower for model2 as compared to model1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh5O8qS_khug",
        "colab_type": "text"
      },
      "source": [
        "Additionally, submit your results [here](https://forms.gle/NtJJEE7Wm5ZRM3Je7) for 'Class specific accuracy of correctly predicting a hit song' and see if you got the best performance of the class! Good luck!"
      ]
    }
  ]
}